{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import (\n",
    "    FunctionTransformer,\n",
    "    LabelEncoder,\n",
    "    MinMaxScaler,\n",
    "    OrdinalEncoder,\n",
    ")\n",
    "\n",
    "from config.config import logger\n",
    "from config.config import ARTIFACTS_DIR\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "df = pd.read_csv(\"data/raw/data.csv\")\n",
    "\n",
    "\n",
    "def create_target(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    def type_of_failure(row_name):\n",
    "        if df.loc[row_name, \"TWF\"] == 1:\n",
    "            df.loc[row_name, \"type_of_failure\"] = \"TWF\"\n",
    "        elif df.loc[row_name, \"HDF\"] == 1:\n",
    "            df.loc[row_name, \"type_of_failure\"] = \"HDF\"\n",
    "        elif df.loc[row_name, \"PWF\"] == 1:\n",
    "            df.loc[row_name, \"type_of_failure\"] = \"PWF\"\n",
    "        elif df.loc[row_name, \"OSF\"] == 1:\n",
    "            df.loc[row_name, \"type_of_failure\"] = \"OSF\"\n",
    "        elif df.loc[row_name, \"RNF\"] == 1:\n",
    "            df.loc[row_name, \"type_of_failure\"] = \"RNF\"\n",
    "\n",
    "    df.apply(lambda row: type_of_failure(row.name), axis=1)\n",
    "    df[\"type_of_failure\"].replace(np.NaN, \"no failure\", inplace=True)\n",
    "    df.drop([\"TWF\", \"HDF\", \"PWF\", \"OSF\", \"RNF\"], axis=1, inplace=True)\n",
    "    encoder = LabelEncoder()\n",
    "    df[\"type_of_failure\"] = encoder.fit_transform(df[\"type_of_failure\"])\n",
    "    logger.info(\"Target variable created\")\n",
    "    return df\n",
    "\n",
    "\n",
    "def convert_to_celsius(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df.drop([\"UDI\", \"Product ID\"], axis=1, inplace=True)\n",
    "    df[\"Air temperature [c]\"] = df[\"Air temperature [K]\"] - 273.15\n",
    "    df[\"Process temperature [c]\"] = df[\"Process temperature [K]\"] - 273.15\n",
    "    df.drop([\"Air temperature [K]\", \"Process temperature [K]\"], axis=1, inplace=True)\n",
    "    logger.info(\"Temperature converted to celsius\")\n",
    "    return df\n",
    "\n",
    "\n",
    "def ordinal_encoding(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    encoder = OrdinalEncoder(categories=[[\"L\", \"M\", \"H\"]])\n",
    "    df[\"Type\"] = encoder.fit_transform(df[[\"Type\"]])\n",
    "    logger.info(\"Type encoded\")\n",
    "    return df\n",
    "\n",
    "\n",
    "def feature_scaling(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    scaler = MinMaxScaler()\n",
    "    scale_cols = [\n",
    "        \"Rotational speed [rpm]\",\n",
    "        \"Torque [Nm]\",\n",
    "        \"Tool wear [min]\",\n",
    "        \"Air temperature [c]\",\n",
    "        \"Process temperature [c]\",\n",
    "    ]\n",
    "    df_scaled = scaler.fit_transform(df[scale_cols])\n",
    "\n",
    "    with open(Path(ARTIFACTS_DIR, \"scaler.pkl\"), \"wb\") as f:\n",
    "        pickle.dump(scaler, f)\n",
    "\n",
    "\n",
    "    df_scaled = pd.DataFrame(df_scaled)\n",
    "    df_scaled.columns = scale_cols\n",
    "\n",
    "    df.drop(scale_cols, axis=1, inplace=True)\n",
    "\n",
    "    df_scaled = pd.concat([df, df_scaled], axis=1)\n",
    "    logger.info(\"Features scaled\")\n",
    "    return df_scaled\n",
    "\n",
    "\n",
    "def sampling(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    X = df.drop([\"type_of_failure\"], axis=1)\n",
    "    y = df[\"type_of_failure\"]\n",
    "    oversample = SMOTE()\n",
    "    X, y = oversample.fit_resample(X, y)\n",
    "    sampled_df = pd.concat([X, y], axis=1)\n",
    "    logger.info(\"Data sampled\")\n",
    "    return sampled_df\n",
    "\n",
    "\n",
    "target_cols = [\"TWF\", \"HDF\", \"PWF\", \"OSF\", \"RNF\"]\n",
    "celsius_cols = [\"UDI\", \"Product ID\", \"Air temperature [K]\", \"Process temperature [K]\"]\n",
    "categorical_cols = [\"Type\"]\n",
    "\n",
    "feature_transformer = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"create_target\", FunctionTransformer(create_target), target_cols),\n",
    "        (\"convert_to_celsius\", FunctionTransformer(convert_to_celsius), celsius_cols),\n",
    "        (\"ordinal_encoding\", FunctionTransformer(ordinal_encoding), [\"Type\"]),\n",
    "    ],\n",
    "    remainder=\"passthrough\",\n",
    ")\n",
    "\n",
    "scaling_transformer = ColumnTransformer(\n",
    "    transformers=[(\"feature_scaling\", MinMaxScaler(), [1, 2, 4, 5, 6])], remainder=\"passthrough\"\n",
    ")\n",
    "\n",
    "\n",
    "def preprocess(df):\n",
    "    pipeline = Pipeline(\n",
    "        steps=[(\"transformer\", feature_transformer), (\"scaling_transformer\", scaling_transformer)]\n",
    "    )\n",
    "\n",
    "    result = pipeline.fit_transform(df)\n",
    "    result = pd.DataFrame(result)\n",
    "\n",
    "    X = result.drop(result.columns[5], axis=1)\n",
    "    y = result[5]\n",
    "\n",
    "    smote = SMOTE(sampling_strategy=\"auto\")\n",
    "    X_resampled, y_resampled = smote.fit_resample(X, y)\n",
    "    result = pd.concat([X_resampled, y_resampled], axis=1)\n",
    "    result.to_csv(\"data/processed/proc_data.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\91789\\anaconda3\\lib\\site-packages\\sklearn\\base.py:347: InconsistentVersionWarning: Trying to unpickle estimator DecisionTreeClassifier from version 1.2.2 when using version 1.3.0. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "node array from the pickle has an incompatible dtype:\n- expected: {'names': ['left_child', 'right_child', 'feature', 'threshold', 'impurity', 'n_node_samples', 'weighted_n_node_samples', 'missing_go_to_left'], 'formats': ['<i8', '<i8', '<i8', '<f8', '<f8', '<i8', '<f8', 'u1'], 'offsets': [0, 8, 16, 24, 32, 40, 48, 56], 'itemsize': 64}\n- got     : [('left_child', '<i8'), ('right_child', '<i8'), ('feature', '<i8'), ('threshold', '<f8'), ('impurity', '<f8'), ('n_node_samples', '<i8'), ('weighted_n_node_samples', '<f8')]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 56\u001b[0m\n\u001b[0;32m     52\u001b[0m     \u001b[38;5;28mprint\u001b[39m(result1, result2)\n\u001b[0;32m     54\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result1, result2\n\u001b[1;32m---> 56\u001b[0m \u001b[43mprediction\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mLow\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1412\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m\t\u001b[49m\u001b[38;5;241;43m52.3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m\t\u001b[49m\u001b[38;5;241;43m218\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m25.15\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m\t\u001b[49m\u001b[38;5;241;43m34.95\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[1], line 9\u001b[0m, in \u001b[0;36mprediction\u001b[1;34m(type, rpm, torque, tool_wear, air_temp, process_temp)\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprediction\u001b[39m(\u001b[38;5;28mtype\u001b[39m, rpm, torque, tool_wear, air_temp, process_temp):\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(Path(ARTIFACTS_DIR,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel1.pkl\u001b[39m\u001b[38;5;124m'\u001b[39m), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m----> 9\u001b[0m         model1 \u001b[38;5;241m=\u001b[39m \u001b[43mpickle\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(Path(ARTIFACTS_DIR,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel2.pkl\u001b[39m\u001b[38;5;124m'\u001b[39m), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m     12\u001b[0m         model2 \u001b[38;5;241m=\u001b[39m pickle\u001b[38;5;241m.\u001b[39mload(f)\n",
      "File \u001b[1;32msklearn\\tree\\_tree.pyx:714\u001b[0m, in \u001b[0;36msklearn.tree._tree.Tree.__setstate__\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32msklearn\\tree\\_tree.pyx:1418\u001b[0m, in \u001b[0;36msklearn.tree._tree._check_node_ndarray\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: node array from the pickle has an incompatible dtype:\n- expected: {'names': ['left_child', 'right_child', 'feature', 'threshold', 'impurity', 'n_node_samples', 'weighted_n_node_samples', 'missing_go_to_left'], 'formats': ['<i8', '<i8', '<i8', '<f8', '<f8', '<i8', '<f8', 'u1'], 'offsets': [0, 8, 16, 24, 32, 40, 48, 56], 'itemsize': 64}\n- got     : [('left_child', '<i8'), ('right_child', '<i8'), ('feature', '<i8'), ('threshold', '<f8'), ('impurity', '<f8'), ('n_node_samples', '<i8'), ('weighted_n_node_samples', '<f8')]"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "from config.config import ARTIFACTS_DIR, logger\n",
    "\n",
    "def prediction(type, rpm, torque, tool_wear, air_temp, process_temp):\n",
    "    with open(Path(ARTIFACTS_DIR,'model1.pkl'), 'rb') as f:\n",
    "        model1 = pickle.load(f)\n",
    "\n",
    "    with open(Path(ARTIFACTS_DIR,'model2.pkl'), 'rb') as f:\n",
    "        model2 = pickle.load(f)\n",
    "\n",
    "    # type preprocessing\n",
    "    if type == 'Low':\n",
    "        type = int(0)\n",
    "    elif type == 'Medium':\n",
    "        type = int(1)\n",
    "    elif type == 'High':\n",
    "        type = int(2)\n",
    "\n",
    "    type = float(type)\n",
    "\n",
    "\n",
    "    # min max scaler\n",
    "    with open(Path(ARTIFACTS_DIR, 'scaler.pkl'), 'rb') as f:\n",
    "        scaler = pickle.load(f)\n",
    "    scaled_input = scaler.transform([[rpm, torque, tool_wear, air_temp, process_temp]])\n",
    "    rpm, torque, tool_wear, air_temp, process_temp = scaled_input[0]\n",
    "\n",
    "    # print(rpm, torque, tool_wear, air_temp, process_temp)\n",
    "\n",
    "    prediction1 = model1.predict([[type, rpm, torque, tool_wear, air_temp, process_temp]])\n",
    "\n",
    "    if prediction1[0] == 0:\n",
    "        result1 = 'No Failure'\n",
    "    elif prediction1[0] == 1:\n",
    "        result1 = 'Machine Failure'\n",
    "    \n",
    "    prediction2 = model2.predict([[type, rpm, torque, tool_wear, air_temp, process_temp]])\n",
    "    prediction2 = int(prediction2)\n",
    "\n",
    "    encoding = {0: 'Heat Dissipation Failure',\n",
    "                1: 'Overstrain Failure',\n",
    "                2: 'Power Failure',\n",
    "                3: 'Random Failure',\n",
    "                4: 'Tool Wear Failure',\n",
    "                5: 'No Failure'}\n",
    "    \n",
    "    result2 = encoding[prediction2]\n",
    "\n",
    "    print(result1, result2)\n",
    "\n",
    "    return result1, result2\n",
    "\n",
    "prediction('Low', 1412,\t52.3,\t218,25.15,\t34.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'convert_to_celsius' from 'data' (unknown location)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 12\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mconfig\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m config\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mconfig\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconfig\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ARTIFACTS_DIR\n\u001b[1;32m---> 12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     13\u001b[0m     convert_to_celsius,\n\u001b[0;32m     14\u001b[0m     create_target,\n\u001b[0;32m     15\u001b[0m     feature_scaling,\n\u001b[0;32m     16\u001b[0m     ordinal_encoding,\n\u001b[0;32m     17\u001b[0m     sampling,\n\u001b[0;32m     18\u001b[0m )\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtrain\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m model1, model2\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01meda\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     21\u001b[0m     setup,\n\u001b[0;32m     22\u001b[0m     question_one,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     27\u001b[0m     question_six\n\u001b[0;32m     28\u001b[0m )\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'convert_to_celsius' from 'data' (unknown location)"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "from pathlib import Path\n",
    "import typer\n",
    "import pickle\n",
    "import json\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from config import config\n",
    "from config.config import ARTIFACTS_DIR\n",
    "from data import (\n",
    "    convert_to_celsius,\n",
    "    create_target,\n",
    "    feature_scaling,\n",
    "    ordinal_encoding,\n",
    "    sampling,\n",
    ")\n",
    "from train import model1, model2\n",
    "from src.eda import (\n",
    "    setup,\n",
    "    question_one,\n",
    "    question_two,\n",
    "    question_three,\n",
    "    question_four,\n",
    "    question_five,\n",
    "    question_six\n",
    ")\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "app = typer.Typer()\n",
    "\n",
    "def get_data():\n",
    "    df = pd.read_csv(\"data/raw/data.csv\")\n",
    "    return df\n",
    "\n",
    "def eda(df):\n",
    "    df = setup(df)\n",
    "    q1 = question_one(df)\n",
    "    q2 = question_two(df)\n",
    "    q3 = question_three(df)\n",
    "    q4 = question_four(df)\n",
    "    q5 = question_five(df)\n",
    "\n",
    "    json_obj = {\n",
    "        \"q1\": q1,\n",
    "        \"q2\": q2,\n",
    "        \"q3\": q3,\n",
    "        \"q4\": q4,\n",
    "        \"q5\": q5\n",
    "    }\n",
    "\n",
    "    with open(Path(ARTIFACTS_DIR, \"eda.json\"), \"w+\") as f:\n",
    "        json.dump(json_obj, f)\n",
    "    \n",
    "# @app.command()\n",
    "def preprocess():\n",
    "    df = pd.read_csv(Path(config.DATA_DIR, \"raw/data.csv\"))\n",
    "    df = create_target(df)\n",
    "    df = convert_to_celsius(df)\n",
    "    df = ordinal_encoding(df)\n",
    "    df = feature_scaling(df)\n",
    "    df = sampling(df)\n",
    "    df.to_csv(Path(config.DATA_DIR, \"processed/preprocessed.csv\"), index=False)\n",
    "    return df\n",
    "\n",
    "# @app.command()\n",
    "def split_data():\n",
    "    df = pd.read_csv(Path(config.DATA_DIR, \"processed/preprocessed.csv\"))\n",
    "    target = 'type_of_failure'\n",
    "    train_data, test_data = train_test_split(df, test_size=0.2, random_state=42, stratify=df[target])\n",
    "    train_data.to_csv(Path(config.DATA_DIR, \"processed/train.csv\"), index=False)\n",
    "    test_data.to_csv(Path(config.DATA_DIR, \"processed/test.csv\"), index=False)\n",
    "\n",
    "\n",
    "# @app.command()\n",
    "def train():\n",
    "    df = pd.read_csv(Path(config.DATA_DIR, \"processed/train.csv\"))\n",
    "    scores_df, best_model, best_model_name, report = model1(df)\n",
    "    print(\"Scores\")\n",
    "    print(scores_df)\n",
    "    print(\"Best model\")\n",
    "    print(best_model)\n",
    "    print(\"Classification report\")\n",
    "    print(report)\n",
    "    with open(Path(ARTIFACTS_DIR, \"model1.pkl\"), \"wb\") as f:\n",
    "        pickle.dump(best_model, f)\n",
    "\n",
    "    scores_df = scores_df.to_json()\n",
    "    report = report.to_json()\n",
    "    model_metrics = [scores_df, report, best_model_name]\n",
    "    with open(Path(ARTIFACTS_DIR, \"model1_metrics.json\"), \"w+\") as f:\n",
    "        json.dump(model_metrics, f)\n",
    "\n",
    "\n",
    "    scores_df, best_model, best_model_name, report = model2(df)\n",
    "    print(\"Scores\")\n",
    "    print(scores_df)\n",
    "    print(\"Best model\")\n",
    "    print(best_model)\n",
    "    print(\"Classification report\")\n",
    "    print(report)\n",
    "\n",
    "    with open(Path(ARTIFACTS_DIR, \"model2.pkl\"), \"wb\") as f:\n",
    "        pickle.dump(best_model, f)\n",
    "\n",
    "    scores_df = scores_df.to_json()\n",
    "    report = report.to_json()\n",
    "    model_metrics = [scores_df, report, best_model_name]\n",
    "    with open(Path(ARTIFACTS_DIR, \"model2_metrics.json\"), \"w+\") as f:\n",
    "        json.dump(model_metrics, f)\n",
    "\n",
    "# @app.command()\n",
    "# def generate_reports():\n",
    "#     data_report()\n",
    "#     model1_report()\n",
    "#     model2_report()\n",
    "    \n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     app()\n",
    "\n",
    "\n",
    "# get_data()\n",
    "eda(get_data())\n",
    "# df = preprocess()\n",
    "# split_data()\n",
    "# print(df)\n",
    "# train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
